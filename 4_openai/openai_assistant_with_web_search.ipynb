{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUyJppjAQpwJV1uNvmG4dQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychoi-kr/llm-api-prog/blob/main/4_openai/openai_assistant_with_web_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTo7UFRQF9w",
        "outputId": "c75eb2fe-1d6e-4b53-cf97-b45836a374f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tavily-python) (2.32.3)\n",
            "Collecting tiktoken>=0.5.1 (from tavily-python)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (2.2.3)\n",
            "Downloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, tavily-python\n",
            "Successfully installed tavily-python-0.5.0 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import json\n",
        "from tavily import TavilyClient"
      ],
      "metadata": {
        "id": "Nc8JnoW3QLpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "tavily_api_key = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "WKv3-PSUQJzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_instructions = \"\"\"\n",
        "You create a glossary entry in Korean on a given term.\n",
        "\n",
        "Use the web_search tool for initial research to gather and verify information from credible sources. This ensures that definitions are informed by the most recent and reliable data.\n",
        "\n",
        "If the tool does not return any information, abort with fail message.\n",
        "\n",
        "Before including a URL, verify its validity and ensure it leads to the specific content being referenced. Avoid using generic homepage URLs unless they directly relate to the content. Never fabricate a fictional URL.\n",
        "\n",
        "Instead of using honorifics (e.g. \"입니다\") in sentences, use haereahe (e.g. \"이다\") to maintain a direct and concise tone.\n",
        "\n",
        "Follow output format below:\n",
        "```\n",
        "[Term]란 [comprehensive definition in 2-3 paragraphs].\n",
        "\n",
        "### 참고\n",
        "\n",
        "{% for each reference %}\n",
        "- {%=reference in APA style. If the author and site name are not the same, write the author and site name separately.}\n",
        "{% end for %}\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3yv5N7FF92YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "IHv1nhx89Ebx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_client = TavilyClient(api_key=tavily_api_key)\n"
      ],
      "metadata": {
        "id": "ISbAJ_xxBQjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_search(query):\n",
        "    search_result = tavily_client.get_search_context(query, search_depth=\"advanced\", max_tokens=8000)\n",
        "    print(search_result)\n",
        "    return search_result"
      ],
      "metadata": {
        "id": "aCqYFlZ4cIuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_json = {\n",
        "    \"name\": \"web_search\",\n",
        "    \"description\": \"Get recent information from the web.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\"type\": \"string\", \"description\": \"The search query to use.\"},\n",
        "        },\n",
        "        \"required\": [\"query\"]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "JFgzxJge0of8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = openai_client.beta.assistants.create(\n",
        "    name=\"Define it!\",\n",
        "    instructions=assistant_instructions,\n",
        "    model=\"gpt-4o\",\n",
        "    tools=[{\"type\": \"function\", \"function\": web_search_json}],\n",
        ")"
      ],
      "metadata": {
        "id": "7p3r_4tW0UwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = openai_client.beta.threads.create()\n",
        "\n",
        "message = openai_client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Large Multimodal Models\",\n",
        ")"
      ],
      "metadata": {
        "id": "CgEXfc1Y0ld2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = openai_client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")"
      ],
      "metadata": {
        "id": "58PIIJN7AXFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    run = openai_client.beta.threads.runs.retrieve(\n",
        "        thread_id=thread.id,\n",
        "        run_id=run.id,\n",
        "    )\n",
        "    run_status = run.status\n",
        "\n",
        "    if run_status == \"requires_action\" and run.required_action is not None:\n",
        "        tools_to_call = run.required_action.submit_tool_outputs.tool_calls\n",
        "        tool_output_array = []\n",
        "        for tool in tools_to_call:\n",
        "            tool_call_id = tool.id\n",
        "            function_name = tool.function.name\n",
        "            function_arg = json.loads(tool.function.arguments)\n",
        "            if function_name == 'web_search':\n",
        "                output = web_search(function_arg[\"query\"])\n",
        "            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n",
        "\n",
        "        run = openai_client.beta.threads.runs.submit_tool_outputs(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "            tool_outputs=tool_output_array,\n",
        "        )\n",
        "    elif run_status in [\"completed\", \"failed\"]:\n",
        "        break\n",
        "\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac3LmP2V_Rzx",
        "outputId": "ad59dcc9-3e8b-4f41-a582-4751b319a0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"[\\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://arxiv.org/abs/2306.14895\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"This tutorial note summarizes the presentation on ``Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023 tutorial on ``Recent Advances in Vision Foundation Models''. The tutorial consists of three parts. We first introduce the background on recent GPT-like large models for vision-and-language modeling to motivate the research in instruction-tuned\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://aclanthology.org/2024.findings-acl.807/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The Revolution of Multimodal Large Language Models: A Survey - ACL Anthology 2024.findings-acl.807 Findings of the Association for Computational Linguistics ACL 2024 In Findings of the Association for Computational Linguistics ACL 2024, pages 13590\\\\\\\\u201313618, Bangkok, Thailand and virtual meeting. The Revolution of Multimodal Large Language Models: A Survey (Caffagni et al., Findings 2024) https://aclanthology.org/2024.findings-acl.807.pdf booktitle = \\\\\\\\\\\\\\\"Findings of the Association for Computational Linguistics ACL 2024\\\\\\\\\\\\\\\", <namePart type=\\\\\\\\\\\\\\\"family\\\\\\\\\\\\\\\">Caffagni</namePart> <identifier type=\\\\\\\\\\\\\\\"doi\\\\\\\\\\\\\\\">10.18653/v1/2024.findings-acl.807</identifier> %S Findings of the Association for Computational Linguistics ACL 2024 [The Revolution of Multimodal Large Language Models: A Survey](https://aclanthology.org/2024.findings-acl.807) (Caffagni et al., Findings 2024) The Revolution of Multimodal Large Language Models: A Survey (Caffagni et al., Findings 2024) In Findings of the Association for Computational Linguistics ACL 2024, pages 13590\\\\\\\\u201313618, Bangkok, Thailand and virtual meeting.\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://research.aimultiple.com/large-multimodal-models/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"To stay up-to-date on B2B tech & accelerate your enterprise:\\\\\\\\nNext to Read\\\\\\\\nComparing 10+ LLMOps Tools: A Comprehensive Vendor Benchmark\\\\\\\\nAn In-depth Guide to Meta LLaMa Language Model in 2024\\\\\\\\nLarge Language Model Evaluation in 2024: 5 Methods\\\\\\\\nComments\\\\\\\\nYour email address will not be published. Large Multimodal Models (LMMs) vs Large Language Models (LLMs)\\\\\\\\nLarge multimodal models (LMMs) represent a significant breakthrough, capable of interpreting diverse data types like text, images, and audio. Pre-Training\\\\\\\\n4- Fine-Tuning\\\\\\\\n5- Evaluation and Iteration\\\\\\\\nWhat are some famous large multimodal models?\\\\\\\\n Also, multimodal language model outputs are targeted to be not only textual but visual, auditory etc.\\\\\\\\nMultimodal language models are considered to be next steps toward artificial general intelligence.\\\\\\\\n Although most multimodal large language models today can only use text and image, future research is directed at including audio and video data inputs.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://arxiv.org/abs/2311.13165\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The exploration of multimodal language models integrates multiple data types, such as images, text, language, audio, and other heterogeneity. While the latest large language models excel in text-based tasks, they often struggle to understand and process other data types. Multimodal models address this limitation by combining various modalities, enabling a more comprehensive understanding of\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://ieeexplore.ieee.org/document/10386743\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The exploration of multimodal language models integrates multiple data types, such as images, text, language, audio, and other heterogeneity. While the latest large language models excel in text-based tasks, they often struggle to understand and process other data types. Multimodal models address this limitation by combining various modalities, enabling a more comprehensive understanding of\\\\\\\"}\\\"]\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_status == 'completed':\n",
        "    messages = openai_client.beta.threads.messages.list(\n",
        "        thread_id=thread.id,\n",
        "    )\n",
        "    print(messages.data[0].content[0].text.value)\n",
        "else:\n",
        "    print(f\"Run status: {run_status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5fze40QAd31",
        "outputId": "12e3778d-734d-40fd-81ec-efda96d77ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Multimodal Models란 서로 다른 데이터 유형을 함께 처리하는 능력을 가진 대형 모델이다. 이러한 모델은 텍스트, 이미지, 오디오 등 다양한 입력 모달리티를 효율적으로 통합하여 보다 복합적인 이해를 가능하게 한다. 기존의 대형 언어 모델은 주로 텍스트 기반 작업에서 우수한 성능을 발휘하는 반면, 비텍스트 데이터의 처리에는 한계가 있었다. 여기서 멀티모달 모델은 여러 종류의 데이터를 결합하여 이러한 한계를 극복하고자 한다.\n",
            "\n",
            "이러한 모델은 인공지능 연구에서 중요한 발전으로 간주되며, 텍스트와 이미지를 넘어서 오디오 및 비디오 데이터 입력까지 포함하는 방향으로 연구가 진행 중이다. 멀티모달 대형 언어 모델의 목표는 단순한 텍스트 출력을 넘어 비주얼 및 청각적 출력을 생성하는 것이다. 이와 같은 기술 발전은 인공지능의 일반적 이해 능력 향상을 위한 중요한 단계로 여겨지고 있다.\n",
            "\n",
            "### 참고\n",
            "\n",
            "- Caffagni et al. (2024). The Revolution of Multimodal Large Language Models: A Survey. ACL Anthology. Retrieved from https://aclanthology.org/2024.findings-acl.807\n",
            "- AIMultiple. (n.d.). Large Multimodal Models vs Large Language Models. Retrieved from https://research.aimultiple.com/large-multimodal-models/\n",
            "- Arxiv. (2023). Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4. Retrieved from https://arxiv.org/abs/2306.14895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client.beta.assistants.delete(assistant.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1rDC6ih1HCl",
        "outputId": "6c5fe72a-0b32-48dc-f546-24ba0ff7bafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AssistantDeleted(id='asst_PsEYtxtMfPlrq2aNpc77ZkVS', deleted=True, object='assistant.deleted')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}