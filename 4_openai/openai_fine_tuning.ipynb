{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSkjPkpyObtZbhlUT9FTwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychoi-kr/LLM-API/blob/main/openai/openai_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIeUH0j3veBa",
        "outputId": "cb5dff20-698e-48be-edf0-5c3ccb4bff2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/262.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/262.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "jQtMnNAUxnLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파일 생성"
      ],
      "metadata": {
        "id": "R_2aI_QQxBOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/storage 에서 업로드하거나 아래 코드를 실행"
      ],
      "metadata": {
        "id": "yHfBjtp0xXY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "I_KqcGpIrrF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOszD0n-rutZ",
        "outputId": "053ca9fe-8d2d-4f1d-9888-a69516a82d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_file_dir = \"/content/drive/MyDrive/llm-api-prog/data/\"\n",
        "training_file_name = \"번역투순화_50례_20240320\""
      ],
      "metadata": {
        "id": "2rVCwRLmsUPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JXy7vjsrdp7",
        "outputId": "6de05497-4193-4259-95d3-f28184dbe16d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileObject(id='file-JLGWuYxbeley6tBbXTxPDvkB', bytes=40298, created_at=1710919428, filename='번역투순화_50례_20240320.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "client.files.create(\n",
        "  file=open(training_file_dir + training_file_name + \".jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"file-JLGWuYxbeley6tBbXTxPDvkB\""
      ],
      "metadata": {
        "id": "XGX138kev4tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 작업 생성"
      ],
      "metadata": {
        "id": "nCFSQDKGxHbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/finetune 에서 생성하거나 아래 코드를 실행"
      ],
      "metadata": {
        "id": "C1NtvByAxSKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.create(\n",
        "  training_file=file_id,\n",
        "  model=\"gpt-3.5-turbo\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-W9fmXZslNL",
        "outputId": "ca8624fa-a500-4039-d0a7-8e6f6e7db0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-OAUwoa0ddecU7XerX2mfjW8R', created_at=1710919589, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-AmRROjbqJiSw6PeceyIUiQHY', result_files=[], status='validating_files', trained_tokens=None, training_file='file-JLGWuYxbeley6tBbXTxPDvkB', validation_file=None, user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 작업 확인"
      ],
      "metadata": {
        "id": "s4KXsTWJxKPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/finetune 에서 확인하거나 아래 코드를 실행"
      ],
      "metadata": {
        "id": "dWQDlj4awil4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List 10 fine-tuning jobs\n",
        "client.fine_tuning.jobs.list(limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVqHpIeAs8gV",
        "outputId": "087571af-c8e2-43eb-b17c-fee5a2bbee31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-OAUwoa0ddecU7XerX2mfjW8R', created_at=1710919589, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-AmRROjbqJiSw6PeceyIUiQHY', result_files=[], status='running', trained_tokens=None, training_file='file-JLGWuYxbeley6tBbXTxPDvkB', validation_file=None, user_provided_suffix=None)], object='list', has_more=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jobname = \"ftjob-OAUwoa0ddecU7XerX2mfjW8R\"\n",
        "\n",
        "# Retrieve the state of a fine-tune\n",
        "client.fine_tuning.jobs.retrieve(jobname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ofBaOOmtKns",
        "outputId": "6717346f-1d98-402a-b5b6-02648868ec98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-OAUwoa0ddecU7XerX2mfjW8R', created_at=1710919589, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-AmRROjbqJiSw6PeceyIUiQHY', result_files=[], status='running', trained_tokens=None, training_file='file-JLGWuYxbeley6tBbXTxPDvkB', validation_file=None, user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 완료 후 모델 확인\n",
        "client.fine_tuning.jobs.retrieve(jobname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4eaGU4C05jY",
        "outputId": "4919df6f-f423-4f5d-e9f7-43d8fb4fdf3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-OAUwoa0ddecU7XerX2mfjW8R', created_at=1710919589, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::94klBSwO', finished_at=1710920084, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-AmRROjbqJiSw6PeceyIUiQHY', result_files=['file-udvAke5IBpYzkURq0ZOAFBwc'], status='succeeded', trained_tokens=40881, training_file='file-JLGWuYxbeley6tBbXTxPDvkB', validation_file=None, user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = \"ft:gpt-3.5-turbo-0125:personal::94klBSwO\""
      ],
      "metadata": {
        "id": "lNhkm2HwtvI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트"
      ],
      "metadata": {
        "id": "5kHiE8mz1j1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 모델\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-0125\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"다음 문장을 자연스러운 한국어로 교정해 주세요.\"},\n",
        "    {\"role\": \"user\", \"content\": \"서비스 지향 아키텍처(service-oriented architecture, SOA)에 대한 아이디어는 새로운 것이 아니지만, 마이크로서비스 등장은 여러 옵션을 제공해주었다. 2000년대 초반에 SOA라는 용어가 도입되어 이미 이를 시도해보았다. 그러나 SOA의 경우, 다양한 요구사항과 제약 사항이 있었다. 통신 프로토콜부터 API 검색까지 사용량이 많은 애플리케이션까지 모든 것이 미리 결정되었거나 최소한 강력하게 권장되었다.\"}\n",
        "  ]\n",
        ")\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xPDPOgD1Yzd",
        "outputId": "7ece6e2e-ad01-4b76-a3e2-2f81c0de7f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='서비스 지향 아키텍처(SOA)에 대한 아이디어는 새로운 것이 아니었지만, 마이크로서비스의 등장으로 다양한 옵션이 제공되었습니다. 2000년대 초반에 SOA라는 용어가 나왔을 때 이미 이를 시도해 보았지만, 그 당시에는 다양한 요구사항과 제약조건이 있었습니다. 통신 프로토콜부터 API 검색까지, 사용빈도가 높은 애플리케이션의 모든 것이 미리 결정되었거나 적어도 강력히 권장되었던 것입니다.', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파인튜닝 모델\n",
        "completion = client.chat.completions.create(\n",
        "  model=finetuned_model,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"다음 문장을 자연스러운 한국어로 교정해 주세요.\"},\n",
        "    {\"role\": \"user\", \"content\": \"서비스 지향 아키텍처(service-oriented architecture, SOA)에 대한 아이디어는 새로운 것이 아니지만, 마이크로서비스 등장은 여러 옵션을 제공해주었다. 2000년대 초반에 SOA라는 용어가 도입되어 이미 이를 시도해보았다. 그러나 SOA의 경우, 다양한 요구사항과 제약 사항이 있었다. 통신 프로토콜부터 API 검색까지 사용량이 많은 애플리케이션까지 모든 것이 미리 결정되었거나 최소한 강력하게 권장되었다.\"}\n",
        "  ]\n",
        ")\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3blFpNsxtjaX",
        "outputId": "49c96d8f-eb8c-432a-9087-0bf118402014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='서비스 지향 아키텍처 인 아이디어는 새롭지 않지만, 마이크로서비스가 등장하면서 여러 옵션을 제공하게 되었다. 2000년대 초반이라고 하면 SOA를 시도해본 시기이다. 하지만, SOA는 당시에 다양한 요구사항과 제약 사항이 있었다. 통신 프로토콜부터 API 검색까지 모든 것이 미리 결정되었거나 최소한 강력하게 권장되었다.', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    }
  ]
}